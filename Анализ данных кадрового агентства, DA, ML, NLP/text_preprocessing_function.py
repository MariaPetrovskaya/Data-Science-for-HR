# -*- coding: utf-8 -*-
"""text_preprocessing function.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pBzCnbA8n2jJiI1ti0WCsxnFHE74eiUF

тут изначально 4 признака из текстовых полей датасета, сливаются в один и заполнялись пропуски, там где они были.
"""



data1['additional']= data1['additional'].fillna('или')
#датасет и так невелик, поэтому нашла выход чем нейтральным заполнить пропуски в "additional"

data1['language'] = data1['mandatory']+data1['work_conditions']+data1['job_title']+data1['additional']

"""дальше при обучении была токенизация и ВЕRT. На всякий случай добавлю их, т.к. не очень уверена что именно может быть подключено в проде.



"""

tokenizer = AutoTokenizer.from_pretrained("ai-forever/sbert_large_mt_nlu_ru")
model = AutoModel.from_pretrained("ai-forever/sbert_large_mt_nlu_ru")



tokenized = data1['language'].apply(
    lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512, truncation=True))

max_len = 0
for i in tokenized.values:
    if len(i) > max_len:
        max_len = len(i)

padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])

attention_mask = np.where(padded != 0, 1, 0)

batch_size = 100

#Этот код ждать выполнения не обязательно, он считался в Colab около 4 часов, я выгрузила его результаты дальше как переменные

batch_size = 100
embeddings = []
for i in notebook.tqdm(range(padded.shape[0] // batch_size)):
        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)])
        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])

        with torch.no_grad():
            batch_embeddings = model(batch, attention_mask=attention_mask_batch)

        #embeddings.append(batch_embeddings[0][:,:].numpy())
        embeddings.append(batch_embeddings[0][:,0,:].numpy())
        del batch
        del attention_mask_batch
        del batch_embeddings

features = np.concatenate(embeddings)